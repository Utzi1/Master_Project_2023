 def train_step(self, data):
    with tf.GradientTape() as tape:
        z_mean, z_logvar, z = self.encoder(data)
        reconstruction = self.decoder(z)
        reconstruction_loss = mean(K.square(data - reconstruction),
                                     axis=1)
        kl_loss = -.5 * (1 + z_logvar - square(z_mean) - exp(
            z_logvar)
                         )
        kl_loss = sum(kl_loss)
        total_loss = kl_loss + reconstruction_loss

    grads = tape.gradient(total_loss, self.trainable_weights)
    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
    self.total_loss_tracker.update_state(total_loss)
    self.reconstruction_loss_tracker.update_state(reconstruction_loss)
    self.kl_loss_tracker.update_state(kl_loss)
    return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),                                  "kl_loss": self.kl_loss_tracker.result()}
